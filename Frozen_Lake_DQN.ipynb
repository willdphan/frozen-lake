{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/willdphan/frozen-lake-dqn/blob/master/Frozen_Lake_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frozen Lake DQN\n",
        "> ## Steps to Frozen Lake DQN:\n",
        "\n",
        ">>[How Frozen Lake Works](#scrollTo=FjGSWK46cmdp)\n",
        "\n",
        ">>[Table Dictionary](#scrollTo=ZXB0VSq4pY6u)\n",
        "\n",
        ">>>[1. Install Libraries](#scrollTo=SEOQXocOcmdq)\n",
        "\n",
        ">>>[2. Create the Environment](#scrollTo=JJxctuPncmdq)\n",
        "\n",
        ">>>[3. Create the Q-table](#scrollTo=lMAviw6Ycmdr)\n",
        "\n",
        ">>>[4. Initialize Q-learning Parameters](#scrollTo=SBPxURG-cmdr)\n",
        "\n",
        ">>>[5. Build the Q-learning Algorithm](#scrollTo=mwm6JljVcmdr)\n",
        "\n",
        ">>>[6. Examine the Rewards](#scrollTo=7n6TL-nlcmdr)\n",
        "\n",
        ">>>[7. Interpret Training Results](#scrollTo=NVqaAAg0cmds)\n",
        "\n",
        ">>>[8. Build the Q-learning Interface](#scrollTo=TGOCqkeVcmds)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "0aQZFcs7mT1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How Frozen Lake Works\n",
        "\n",
        "This grid is our environment where `S` is the agent's starting point, and it's safe. `F` represents the frozen surface and is also safe. `H` represents a hole, and if our agent steps in a hole in the middle of a frozen lake, well, that's not good. Finally, `G` represents the goal, which is the space on the grid where the prized frisbee is located.\n",
        "\n",
        "The agent can navigate left, right, up, and down, and the episode ends when the agent reaches the goal or falls in a hole. It receives a reward of one if it reaches the goal, and zero otherwise."
      ],
      "metadata": {
        "id": "FjGSWK46cmdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Table Dictionary\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "  <tr>\n",
        "    <th>State</th>\n",
        "    <th>Description</th>\n",
        "    <th>Reward</th>\n",
        "  </tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "  <tr>\n",
        "    <td>S</td>\n",
        "    <td>Agent's starting point - safe</td>\n",
        "    <td>0</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>F</td>\n",
        "    <td>Frozen surface - safe</td>\n",
        "    <td>0</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>H</td>\n",
        "    <td>Hole - game over</td>\n",
        "    <td>0</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>G</td>\n",
        "    <td>Goal - game over</td>\n",
        "    <td>1</td>\n",
        "  </tr>\n",
        "</tbody>\n",
        "</table>"
      ],
      "metadata": {
        "id": "ZXB0VSq4pY6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Install Libraries"
      ],
      "metadata": {
        "id": "SEOQXocOcmdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-25T21:15:36.289968Z",
          "iopub.execute_input": "2023-06-25T21:15:36.290217Z",
          "iopub.status.idle": "2023-06-25T21:15:36.738704Z",
          "shell.execute_reply.started": "2023-06-25T21:15:36.290187Z",
          "shell.execute_reply": "2023-06-25T21:15:36.737671Z"
        },
        "trusted": true,
        "id": "1Jah8OfZcmdq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Create the Environment"
      ],
      "metadata": {
        "id": "JJxctuPncmdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-25T21:15:36.740783Z",
          "iopub.execute_input": "2023-06-25T21:15:36.741034Z",
          "iopub.status.idle": "2023-06-25T21:15:36.755165Z",
          "shell.execute_reply.started": "2023-06-25T21:15:36.741005Z",
          "shell.execute_reply": "2023-06-25T21:15:36.753880Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGKv0H15cmdq",
        "outputId": "95107707-a964-4696-8291-f9dbb55accb7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Create the Q-table\n",
        "Retrieve the size of the space and initialize the q-table with the sizes.\n",
        "\n",
        "`env.action_space.n` returns the number of possible actions in the environment. In the case of CartPole-v1, there are two possible actions:\n",
        "\n",
        "1.   more right\n",
        "2.   move left\n",
        "3.   move down\n",
        "4.   move up\n",
        "\n",
        "Therefore, action_space_size will be equal to 4.\n",
        "\n",
        "`env.observation_space.n` returns the number of possible states in the environment. This is continuous."
      ],
      "metadata": {
        "id": "lMAviw6Ycmdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "action_space_size = env.action_space.n # = 4 actions\n",
        "state_space_size = env.observation_space.n # = 16 states, 16 tiles\n",
        "action_space_size, state_space_size"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-25T21:15:36.756840Z",
          "iopub.execute_input": "2023-06-25T21:15:36.757371Z",
          "iopub.status.idle": "2023-06-25T21:15:36.761204Z",
          "shell.execute_reply.started": "2023-06-25T21:15:36.757330Z",
          "shell.execute_reply": "2023-06-25T21:15:36.760181Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dlPhekfcmdr",
        "outputId": "8e345e67-9b7c-40fa-a682-b0f61021350d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# q_table takes in state space size and action space size\n",
        "q_table = np.zeros((state_space_size, action_space_size))\n",
        "q_table"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-25T21:15:36.762554Z",
          "iopub.execute_input": "2023-06-25T21:15:36.762870Z",
          "iopub.status.idle": "2023-06-25T21:15:36.779401Z",
          "shell.execute_reply.started": "2023-06-25T21:15:36.762838Z",
          "shell.execute_reply": "2023-06-25T21:15:36.778741Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ3EEDKrcmdr",
        "outputId": "0508c281-15d4-4784-91d1-0e7a89420a52"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Initialize Q-learning Parameters\n",
        "\n",
        "Define and initialize the parameters used in the Q-learning algorithm for training an agent in the CartPole-v1 environment."
      ],
      "metadata": {
        "id": "SBPxURG-cmdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1000 episodes\n",
        "num_episodes = int(1e4)\n",
        "# The maximum number of steps allowed per episode. If the agent reaches this limit\n",
        "# without completing the task (balancing the pole), the episode will end.\n",
        "max_steps_per_episode = 100\n",
        "\n",
        "# rate at which agents update Q-values. Low rate of 0.1. Small updates.\n",
        "lr = 1e-1\n",
        "# determines the importance of future rewards compared to immediate rewards.\n",
        "# It allows the agent to prioritize long-term rewards over short-term rewards.\n",
        "dr = 0.99\n",
        "\n",
        "# probability of agent choosing a random action instead of exploiting learned Q-values.\n",
        "# Initially set to 1, agent explores the environment extensively to gather information.\n",
        "# As training progresses, exploration rate decays, agent relies more on learned Q-values for decisions.\n",
        "exploration_rate = 1\n",
        "# upperbound for exploration rate\n",
        "max_exp_rate = 1\n",
        "# lowerbound for exploration rate\n",
        "min_exp_rate = 1e-3\n",
        "# rate at which the exploration rate decays over time. It controls the rate of\n",
        "# reduction in the exploration rate as the agent gains more experience.\n",
        "exp_decay_rate = 1e-3\n",
        "\n",
        "# building a reward list for all the episodes\n",
        "rewards_all_episodes = []"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-25T21:15:36.780739Z",
          "iopub.execute_input": "2023-06-25T21:15:36.781529Z",
          "iopub.status.idle": "2023-06-25T21:15:36.791970Z",
          "shell.execute_reply.started": "2023-06-25T21:15:36.781483Z",
          "shell.execute_reply": "2023-06-25T21:15:36.791155Z"
        },
        "trusted": true,
        "id": "Yvca9jczcmdr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Build the Q-learning Algorithm\n",
        "\n",
        "The code trains an agent using the Q-learning algorithm in the CartPole-v1 environment. It runs a loop for each episode, where the agent takes actions based on an epsilon-greedy strategy. The Q-table is updated using the Bellman equation, combining immediate rewards with expected future rewards. The exploration rate gradually decreases over episodes. The total rewards for each episode are stored for analysis. The goal is to train the agent to balance the pole on the cart for as long as possible.\n",
        "TLDR;\n",
        "1.    The code executes a loop for each episode, where an episode represents a complete playthrough of the game. `rewards_current_episode`, is initialized to keep track of the cumulative rewards obtained in the current episode.\n",
        "2.    Within each episode, a loop runs for each step the agent takes.\n",
        "  *   Choose an action based on the epsilon-greedy strategy: exploit the maximum. Q-value or explore by selecting a random action.\n",
        "  *   Take the chosen action and receive feedback: new state, reward, done flag.\n",
        "  *   Update the Q-table based on the Bellman equation, combining old Q-values with learned values.\n",
        "  *   Transition to the new state and add the immediate reward to cumulative rewards\n",
        "  *   Check if the episode is done, and if so, break the loop for the current episode and move to the next episode. Otherwise, transition to the next time-step within the episode\n",
        "3. Then, we use the Exploration Rate decay to decrease the exploraion rate over episodes and keep track of the total rewards to keep track of the agent's performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "mwm6JljVcmdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-learning algorithm\n",
        "# for-loop for each episode\n",
        "for episode in range(num_episodes):\n",
        "\n",
        "    # reset the movements in the env\n",
        "    state = env.reset()\n",
        "    # check if the agent reaches the target\n",
        "    done = False\n",
        "    # variable for expected return G_t\n",
        "    rewards_current_episode = 0\n",
        "\n",
        "    # for loop for each step for the agent\n",
        "    for step in range(max_steps_per_episode):\n",
        "        # apply epsilon greedy stategy. generates a random floating-point number between 0 and 1.\n",
        "        random_number = random.uniform(0, 1)\n",
        "        # Exploration Vs. Exploitation trade-off. Used in the epsilon-greedy strategy\n",
        "        if random_number > exploration_rate:\n",
        "            # start exploitation -> getting the maximum Q-value from the possible movements of his current state.\n",
        "            action = np.argmax(q_table[state, :])\n",
        "        else:\n",
        "            # start exploration -> select any random action to explore a random state.\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        # after taking the action, we're going to update our agent with the new\n",
        "          # current state, rewards, if it reached the end or not, info\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update our Q-table for Q(s, a) using Bellman Equation\n",
        "                                  # weight of lr  * Old Q-value\n",
        "        q_table[state, action] = (1 - lr) * q_table[state, action] + lr * (reward + dr*(np.max(q_table[new_state, :])))\n",
        "                            # lr * (immediate reward + dsicount rate(maximum Q-value among all possible actions in the new state))\n",
        "                            # AKA learned value\n",
        "\n",
        "        # transition to the next state\n",
        "        state = new_state\n",
        "        # accumulates the rewards obtained by the agent in the current episode.\n",
        "        rewards_current_episode += reward\n",
        "\n",
        "        # check to see if our last action ended the episode for us,\n",
        "        # meaning, did our agent step in a hole or reach the goal?\n",
        "        if done:\n",
        "            break\n",
        "        # If the action did end the episode, then we jump out of this loop and move on to the next episode.\n",
        "        # Otherwise, we transition to the next time-step.\n",
        "\n",
        "    # Exploration Rate Decay, used to decrease the exploration rate over episodes.\n",
        "    # https://en.wikipedia.org/wiki/Exponential_decay\n",
        "    exploration_rate = min_exp_rate + (max_exp_rate - min_exp_rate) * np.exp(-exp_decay_rate * episode)\n",
        "\n",
        "    # append the current rewards in the list of rewards.\n",
        "    # allows for the analysis and evaluation of the agent's performance over multiple episodes\n",
        "    rewards_all_episodes.append(rewards_current_episode)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-25T21:15:36.793318Z",
          "iopub.execute_input": "2023-06-25T21:15:36.793785Z",
          "iopub.status.idle": "2023-06-25T21:15:45.247605Z",
          "shell.execute_reply.started": "2023-06-25T21:15:36.793743Z",
          "shell.execute_reply": "2023-06-25T21:15:45.245901Z"
        },
        "trusted": true,
        "id": "sQ-E7hqMcmdr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Examine the Rewards\n",
        "From the printout we can notice that our average reward per thousand epoisodes did indeed progress overtime. When the algorithm first start training, the first thousands episodes only average a reward of `0.062`, but by the time it got to its last thousand episodes, the reward improved to `0.74`.\n",
        "\n",
        "Calculate the average reward per thousand episodes\n",
        "`np.split` splits the NumPy array of rewards into sub-arrays of equal size, where each sub-array which represents the cumulative rewards obtained over a thousand episodes. `np.array` converts to np for easier manipulation. array calculates the number of groups (or splits) based on the total `num_episodes / 1000`. This determines the number of sub-arrays to create.\n",
        "\n",
        "Print an output that displays the average rewards per thousand episodes for each group in the `rewards_per_thousand_episodes` list. The count number indicates the range of episodes covered in each group, and the average reward represents the average reward obtained per episode within that range."
      ],
      "metadata": {
        "id": "7n6TL-nlcmdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\n",
        "count = 1000\n",
        "\n",
        "print(\"Average rewards per thousand episodes\".center(100, '*'))\n",
        "for reward in rewards_per_thousand_episodes:\n",
        "    print(f'Count No. {count:,}: {sum(reward/1000)}')\n",
        "    count += 1000"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-25T21:15:45.249209Z",
          "iopub.execute_input": "2023-06-25T21:15:45.249627Z",
          "iopub.status.idle": "2023-06-25T21:15:45.262815Z",
          "shell.execute_reply.started": "2023-06-25T21:15:45.249562Z",
          "shell.execute_reply": "2023-06-25T21:15:45.261885Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7dZK-dlcmdr",
        "outputId": "a9c68268-bbe5-47b0-c97f-3381c61740fe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******************************Average rewards per thousand episodes********************************\n",
            "Count No. 1,000: 0.04900000000000004\n",
            "Count No. 2,000: 0.21900000000000017\n",
            "Count No. 3,000: 0.4070000000000003\n",
            "Count No. 4,000: 0.5950000000000004\n",
            "Count No. 5,000: 0.6560000000000005\n",
            "Count No. 6,000: 0.6980000000000005\n",
            "Count No. 7,000: 0.6760000000000005\n",
            "Count No. 8,000: 0.7270000000000005\n",
            "Count No. 9,000: 0.7130000000000005\n",
            "Count No. 10,000: 0.7430000000000005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Interpret Training Results\n",
        "\n",
        "Our agent played `10,000` episodes. At each time step within an episode, the agent received a reward of `1` if it reached the frisbee, otherwise, it received a reward of `0`. If the agent did indeed reach the frisbee, then the episode finished at that time-step.\n",
        "\n",
        "So, that means for each episode, the total reward received by the agent for the entire episode is either `1` or `0`. So, for the first thousand episodes, we can interpret this score as meaning that  **6%** of the time, the agent received a reward of `1` and won the episode. And by the last thousand episodes from a total of 10,000, the agent was winning **74%** of the time.\n",
        "\n",
        "The numbers within the square brackets represent the Q-values for the respective actions that can be taken in that state.\n",
        "For example, the first row `[0.58230461 0.48825684 0.48921769 0.49125568] `represents the Q-values for the four possible actions `(0, 1, 2, and 3)` in the first state.\n",
        "\n",
        "Higher Q-values generally indicate a higher expected cumulative reward for taking that action in the given state.\n",
        "\n",
        "In some rows, all Q-values are zero, indicating that the agent has not yet explored or learned about the rewards associated with those state-action pairs.\n",
        "\n",
        "As the agent interacts with the environment and learns through the Q-learning algorithm, the Q-values are updated based on observed rewards and the agent's exploration-exploitation strategy."
      ],
      "metadata": {
        "id": "NVqaAAg0cmds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the Q-Table below\n",
        "print(\"Q-Table\".center(100, '*'))\n",
        "print()\n",
        "\n",
        "for row in q_table:\n",
        "    print(' '* 25, row)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-25T21:15:45.264941Z",
          "iopub.execute_input": "2023-06-25T21:15:45.265988Z",
          "iopub.status.idle": "2023-06-25T21:15:45.289424Z",
          "shell.execute_reply.started": "2023-06-25T21:15:45.265912Z",
          "shell.execute_reply": "2023-06-25T21:15:45.287669Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlkTYMNUcmds",
        "outputId": "de8cac5f-4a70-4bc8-beef-b82d8e90682c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********************************************Q-Table***********************************************\n",
            "\n",
            "                          [0.58230461 0.48825684 0.48921769 0.49125568]\n",
            "                          [0.29350113 0.25999836 0.28457958 0.44168369]\n",
            "                          [0.39210816 0.20972835 0.24604732 0.27011584]\n",
            "                          [0.05790049 0.10516014 0.03315454 0.0606654 ]\n",
            "                          [0.60679958 0.46834759 0.43509862 0.30892254]\n",
            "                          [0. 0. 0. 0.]\n",
            "                          [0.31120102 0.16868329 0.12795261 0.13501385]\n",
            "                          [0. 0. 0. 0.]\n",
            "                          [0.37516465 0.43596654 0.37695086 0.6627882 ]\n",
            "                          [0.46724687 0.71528521 0.35877242 0.39663325]\n",
            "                          [0.71508712 0.34140213 0.36528914 0.26098435]\n",
            "                          [0. 0. 0. 0.]\n",
            "                          [0. 0. 0. 0.]\n",
            "                          [0.45429822 0.49409317 0.78671695 0.48190456]\n",
            "                          [0.738313   0.88171029 0.73472903 0.73903743]\n",
            "                          [0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Build the Q-learning Interface\n",
        "\n",
        "Let's see how interactively the agent plays Frozen Lake. See the agent's interactions with the environment, the environment's visual display, and messages indicating whether the agent reached the goal or fell through a hole. The code provides a step-by-step visualization of the agent's behavior in the environment.\n"
      ],
      "metadata": {
        "id": "TGOCqkeVcmds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(5):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    print(f'Episode: {episode+1}'.center(50, '='))\n",
        "    time.sleep(1)\n",
        "\n",
        "    for step in range(max_steps_per_episode):\n",
        "        # reset env and indicate episode is not complete\n",
        "        # for clearning the board\n",
        "        clear_output(wait=True)\n",
        "        # allows you to check the agent's environment\n",
        "        env.render()\n",
        "        time.sleep(0.4)\n",
        "\n",
        "        # invoke the action (state) with the highest Q-value from the Q-Table for the current state\n",
        "        action = np.argmax(q_table[state, :])\n",
        "\n",
        "        # take the action and move to the new state\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # acting condition\n",
        "        # Checks if the episode is completed. If done is True, the agent has either reached the goal or fallen through a hole.\n",
        "        if done:\n",
        "            # Clears the output in the console.\n",
        "            clear_output(wait=True)\n",
        "            env.render()\n",
        "            if reward == 1:\n",
        "                # Pauses the execution for 3 seconds to display the final state and reward message.\n",
        "                print('You reach the goal!'.center(50, '*'))\n",
        "                time.sleep(3)\n",
        "            else:\n",
        "                # Pauses the execution for 3 seconds to display the final state and reward message.\n",
        "                print('You fall through a hole!'.center(50, '-'))\n",
        "                time.sleep(3)\n",
        "                # Clears the output in the console.\n",
        "                clear_output(wait=True)\n",
        "            break\n",
        "\n",
        "        # select the new state based on the agent action\n",
        "        state = new_state\n",
        "\n",
        "# close the environment\n",
        "env.close()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-25T21:15:45.291079Z",
          "iopub.execute_input": "2023-06-25T21:15:45.291410Z",
          "iopub.status.idle": "2023-06-25T21:17:50.999275Z",
          "shell.execute_reply.started": "2023-06-25T21:15:45.291364Z",
          "shell.execute_reply": "2023-06-25T21:17:50.998548Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npq3oGBgcmds",
        "outputId": "593dde29-339f-4d5a-b922-58534ee83df0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------You fall through a hole!-------------\n"
          ]
        }
      ]
    }
  ]
}